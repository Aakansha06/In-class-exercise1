{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "INFO5731_Assignment_Three.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Three**\n",
        "\n",
        "In this assignment, you are required to conduct information extraction, semantic analysis based on **the dataset you collected from assignment two**. You may use scipy and numpy package in this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1: Understand N-gram**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(45 points). Write a python program to conduct N-gram analysis based on the dataset in your assignment two:\n",
        "\n",
        "(1) Count the frequency of all the N-grams (N=3).\n",
        "\n",
        "(2) Calculate the probabilities for all the bigrams in the dataset by using the fomular count(w2 w1) / count(w2). For example, count(really like) / count(really) = 1 / 3 = 0.33.\n",
        "\n",
        "(3) Extract all the **noun phrases** and calculate the relative probabilities of each review in terms of other reviews (abstracts, or tweets) by using the fomular frequency (noun phrase) / max frequency (noun phrase) on the whole dataset. Print out the result in a table with column name the all the noun phrases and row name as all the 100 reviews (abstracts, or tweets). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuFPKhC0m1fd",
        "outputId": "4004487b-a965-40e2-cfc0-de257c6ab00e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# Write your code here\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import csv\n",
        "import spacy\n",
        "import io as stringIOModule\n",
        "import re\n",
        "import bs4\n",
        "from bs4 import BeautifulSoup as soup\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from urllib.request import urlopen as uReq\n",
        "\n",
        "#nltk.download('punkt')\n",
        "#nltk.download('stopwords')\n",
        "\n",
        "data = pd.read_csv('/posTag.csv', low_memory = False)\n",
        "print(data)\n",
        "\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    Cleaned_title\n",
            "0        abstract\n",
            "1           found\n",
            "2         describ\n",
            "3          method\n",
            "4         statist\n",
            "..            ...\n",
            "219       languag\n",
            "220       process\n",
            "221           nlp\n",
            "222       concern\n",
            "223       develop\n",
            "\n",
            "[224 rows x 1 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsEr3KW26bxG",
        "outputId": "ae0c51db-64bb-4ee9-e854-292c74fc0e00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "word_vectorizer = CountVectorizer(stop_words=None)\n",
        "sparse_matrix = word_vectorizer.fit_transform(data['Cleaned_title'].values)\n",
        "frequencies = sum(sparse_matrix).toarray()[0]\n",
        "df = pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['Frequency'])\n",
        "print(df)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "            Frequency\n",
            "abil                1\n",
            "abl                 1\n",
            "abstract            2\n",
            "access              1\n",
            "accomplish          1\n",
            "...               ...\n",
            "versatil            1\n",
            "virtual             1\n",
            "way                 3\n",
            "work                1\n",
            "yet                 1\n",
            "\n",
            "[129 rows x 1 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6qeO-fl7CXW",
        "outputId": "c9ad75ea-c715-4357-a45c-7bb1dbb08055",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "\n",
        "with open('/posTag.csv') as a:\n",
        "  read = csv.reader(a)\n",
        "  next(read, None)\n",
        "  Cleaned_title = [row[0] for row in read]\n",
        "with open('posTag.txt', mode=\"w\") as o:\n",
        "  for i in Cleaned_title:\n",
        "    o.write(\"%s\\n\" % i)\n",
        "def bigram(file):\n",
        "  l = []\n",
        "  b = {}\n",
        "  u = {}\n",
        "  t = open(file, 'r').read()\n",
        "  l = t.strip().split()\n",
        "  del t\n",
        "  for f in l:\n",
        "    if not f in u:\n",
        "      u[f] = 1\n",
        "    else:\n",
        "      u[f] += 1\n",
        "  for p in range(len(l) - 1):\n",
        "    temp = (l[p], l[p+1])\n",
        "    if not temp in b:\n",
        "      b[temp] = 1\n",
        "    else:\n",
        "      b[temp] += 1\n",
        "  print('Generated', len(b), 'bigrams')\n",
        "  total = sum(u.values())\n",
        "  for n,t in b.items():\n",
        "    first_word = n[0]\n",
        "    first_wordcount = u[first_word]\n",
        "    bi = {}\n",
        "    bi = b[n]/u[first_word]\n",
        "    if t == 2:\n",
        "      print(n[0],n[1],t,bi)\n",
        "bigram('posTag.txt')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generated 193 bigrams\n",
            "maximum entropi 2 1.0\n",
            "field natur 2 1.0\n",
            "term condit 2 1.0\n",
            "process nlp 2 0.16666666666666666\n",
            "descript logic 2 1.0\n",
            "syntact semant 2 1.0\n",
            "semant interpret 2 0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zA0mTBa7Dx4",
        "outputId": "f711999f-eae1-4b8d-c473-f32799e6a451",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        }
      },
      "source": [
        "nlp = spacy.load(\"en\")\n",
        "filename = open(\"posTag.txt\")\n",
        "d = nlp(filename.read())\n",
        "noun = []\n",
        "for p in d.noun_chunks:\n",
        "  noun.append(p.text)\n",
        "df = pd.DataFrame(noun, columns=['noun'])\n",
        "word_vectorizer = CountVectorizer(ngram_range=(3,3), analyzer='word')\n",
        "sparse_matrix = word_vectorizer.fit_transform(df['noun'].values.astype('U'))\n",
        "frequencies = sum(sparse_matrix).toarray()[0]\n",
        "df1 = pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(),columns=['Frequency'])\n",
        "df1['NounProbabilities'] = df1/df1.max()\n",
        "print(noun)\n",
        "df1"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['abstract', 'describ\\nmethod', 'statist', 'model\\nbase', 'maximum', 'entropi', 'maximumlikelihood', 'approach\\nautomat', 'maximum', 'entropi\\nmodel', 'approach', 'effici', 'sever', 'problem', 'natur\\nlanguag\\nprocess\\nscale', 'condit', 'field', 'natur', 'languag\\nprocess\\nterm', 'condit\\nterm', 'condit', 'copyright\\nwork', 'deposit', 'minerva', 'access', 'retain\\npaper', 'address', 'issu', 'cooper', 'linguist', 'languag\\nprocess', 'nlp', 'gener', 'linguist', 'machin', 'translat', 'mt', 'focus', 'cooper\\nname', 'applic', 'linguist', 'nlp', 'natur', 'languag\\nprocess', 'descript', 'encod', 'knowledg\\nbase', 'syntact', 'semant', 'pragmat', 'element', 'semant', 'interpret', 'natur', 'languag', 'gener\\nprocess', 'descript\\nlogic', 'propos\\nunifi', 'neural', 'network\\narchitectur', 'algorithm', 'variou', 'natur\\nlanguag\\nprocess', 'task', 'partofspeech', 'tag', 'chunk', 'name', 'entiti', 'recognit\\nsemant', 'role', 'label', 'versatil', 'achiev', 'tri', 'task', 'natur\\nlanguag\\nprocess', 'natur\\nlanguag\\nprocess', 'consid', 'sens', 'sens\\ncover\\nprocess', 'issu\\nlevel', 'natur', 'languag', 'includ', 'speech', 'recognit', 'syntact', 'semant', 'analysi', 'human\\nfacetofac', 'use', 'natur', 'languag', 'respons', 'way', 'human', 'languag', 'propos\\npsychologicallyinspir', 'natur', 'languag\\nprocess', 'system\\nrobot', 'increment', 'semant', 'interpret', 'natur', 'languag', 'languag', 'human', 'current', 'languag', 'unprocess', 'form', 'comput', 'natur', 'techniqu', 'employ', 'tri\\naccomplish', 'goal\\nfield', 'natur', 'ambigu', 'abil', 'mean', 'way', 'natur', 'languag', 'ambigu', 'comput', 'abl', 'languag', 'way', 'peopl', 'natur\\nlanguag\\nprocess', 'nlp\\nconcern']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Frequency</th>\n",
              "      <th>NounProbabilities</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>languag process scale</th>\n",
              "      <td>1</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>languag process term</th>\n",
              "      <td>1</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>natur languag process</th>\n",
              "      <td>5</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sens cover process</th>\n",
              "      <td>1</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       Frequency  NounProbabilities\n",
              "languag process scale          1                0.2\n",
              "languag process term           1                0.2\n",
              "natur languag process          5                1.0\n",
              "sens cover process             1                0.2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2: Undersand TF-IDF and Document representation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(40 points). Starting from the documents (all the reviews, or abstracts, or tweets) collected for assignment two, write a python program: \n",
        "\n",
        "(1) To build the **documents-terms weights (tf*idf) matrix bold text**.\n",
        "\n",
        "(2) To rank the documents with respect to query (design a query by yourself, for example, \"An Outstanding movie with a haunting performance and best character development\") by using **cosine similarity**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vATjQNTY8buA",
        "outputId": "80e40aa7-3e4e-49d1-81eb-c0ec605230fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from nltk.corpus import stopwords \n",
        "data = pd.read_csv(\"/posTag.csv\")\n",
        "tfidf2 = data.dropna()\n",
        "tfidf1 = (tfidf2['Cleaned_title'].apply(lambda z:pd.value_counts(z.split(\" \"))).sum(axis=0).reset_index())\n",
        "tfidf1.columns = ['words','tf']\n",
        "for l , word in enumerate(tfidf1['words']):\n",
        "  tfidf1.loc[l,'idf'] = np.log(data.shape[0]/(len(tfidf2[tfidf2['Cleaned_title'].str.contains(word)])))\n",
        "tfidf1['tf*idf'] = tfidf1['tf'] * tfidf1['idf']\n",
        "tfidf1"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>words</th>\n",
              "      <th>tf</th>\n",
              "      <th>idf</th>\n",
              "      <th>tf*idf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>abstract</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.718499</td>\n",
              "      <td>9.436998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>found</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.411646</td>\n",
              "      <td>5.411646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>describ</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.718499</td>\n",
              "      <td>9.436998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>method</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.411646</td>\n",
              "      <td>5.411646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>statist</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.411646</td>\n",
              "      <td>5.411646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>mean</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.411646</td>\n",
              "      <td>5.411646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>abl</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.411646</td>\n",
              "      <td>5.411646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>peopl</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.411646</td>\n",
              "      <td>5.411646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>concern</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.411646</td>\n",
              "      <td>5.411646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <td>develop</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.411646</td>\n",
              "      <td>5.411646</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>129 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        words   tf       idf    tf*idf\n",
              "0    abstract  2.0  4.718499  9.436998\n",
              "1       found  1.0  5.411646  5.411646\n",
              "2     describ  2.0  4.718499  9.436998\n",
              "3      method  1.0  5.411646  5.411646\n",
              "4     statist  1.0  5.411646  5.411646\n",
              "..        ...  ...       ...       ...\n",
              "124      mean  1.0  5.411646  5.411646\n",
              "125       abl  1.0  5.411646  5.411646\n",
              "126     peopl  1.0  5.411646  5.411646\n",
              "127   concern  1.0  5.411646  5.411646\n",
              "128   develop  1.0  5.411646  5.411646\n",
              "\n",
              "[129 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWLk38vXR667",
        "outputId": "6cb9c8a7-db12-41c9-bf9f-3d47d6072c5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        }
      },
      "source": [
        "import numpy.linalg as alg\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "train = tfidf2['Cleaned_title'].values.tolist()\n",
        "test = [\"real world problems for concepts in statistical estimation and recognition of pattern\"]\n",
        "stop = stopwords.words('english')\n",
        "vectorizer = CountVectorizer(stop_words = stop)\n",
        "transformer = TfidfTransformer()\n",
        "array = vectorizer.fit_transform(train).toarray()\n",
        "tarray = vectorizer.transform(test).toarray()\n",
        "ab = lambda c,d : np.inner(c,d)/(alg.norm(c)*alg.norm(d))\n",
        "result = []\n",
        "for x in array:\n",
        "  for v in tarray:\n",
        "    cosine = ab(x, v)\n",
        "    result.append(cosine)\n",
        "d = tfidf2.filter(['No','Cleaned_title'], axis=1)\n",
        "s = pd.Series(result)\n",
        "d['Cosine_similarity'] = s.values\n",
        "d.drop(d.loc[d['Cosine_similarity']==0].index, inplace=True)\n",
        "d[\"Rank\"] = d[\"Cosine_similarity\"].rank().astype(float)\n",
        "d.sort_values(\"Cosine_similarity\", inplace=True)\n",
        "d"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in true_divide\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cleaned_title</th>\n",
              "      <th>Cosine_similarity</th>\n",
              "      <th>Rank</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>abstract</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>found</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>describ</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>method</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>statist</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>219</th>\n",
              "      <td>languag</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220</th>\n",
              "      <td>process</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>221</th>\n",
              "      <td>nlp</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>222</th>\n",
              "      <td>concern</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223</th>\n",
              "      <td>develop</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>224 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Cleaned_title  Cosine_similarity  Rank\n",
              "0        abstract                NaN   NaN\n",
              "1           found                NaN   NaN\n",
              "2         describ                NaN   NaN\n",
              "3          method                NaN   NaN\n",
              "4         statist                NaN   NaN\n",
              "..            ...                ...   ...\n",
              "219       languag                NaN   NaN\n",
              "220       process                NaN   NaN\n",
              "221           nlp                NaN   NaN\n",
              "222       concern                NaN   NaN\n",
              "223       develop                NaN   NaN\n",
              "\n",
              "[224 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3: Create your own training and evaluation data for sentiment analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(15 points). **You dodn't need to write program for this question!** Read each review (abstract or tweet) you collected in detail, and annotate each review with a sentiment (positive, negative, or neutral). Save the annotated dataset into a csv file with three columns (first column: document_id, clean_text, sentiment), upload the csv file to GitHub and submit the file link blew. This datset will be used for assignment four: sentiment analysis and text classification. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfvMKJjIXS5G"
      },
      "source": [
        "# The GitHub link of your final csv file\n",
        "\n",
        "# Link: https://github.com/Aakansha06/In-class-exercise1/blob/master/Sentiment%20Analysis.csv"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}